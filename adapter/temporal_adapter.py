# temporal_adapter.py
import argparse
import os
import math
from typing import List, Dict, Tuple

import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm

from transformers import AutoTokenizer, T5EncoderModel
from diffusers import CogVideoXPipeline, CogVideoXDPMScheduler
from diffusers.utils import export_to_video


# =========================
# 1) Temporal Controller
# =========================
class TemporalController(nn.Module):
    r"""
    è¾“å…¥å½’ä¸€åŒ–å¸§ç´¢å¼•/æ—¶é—´æ ‡é‡ tau \in [0,1]ï¼Œè¾“å‡ºé—¨æ§ç³»æ•° gamma(tau) \in [gamma_min, gamma_max]
    è½»é‡ MLPï¼Œå¯æ¢ 1D conv/å° transformer
    """
    def __init__(self, hidden=16, gamma_min=0.5, gamma_max=1.5):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(1, hidden),
            nn.SiLU(),
            nn.Linear(hidden, hidden),
            nn.SiLU(),
            nn.Linear(hidden, 1),
        )
        self.gamma_min = gamma_min
        self.gamma_max = gamma_max

    def forward(self, tau: torch.Tensor):
        """
        tau: [B,1] æˆ– [1,1]ï¼Œæ•°å€¼èŒƒå›´å»ºè®® [0,1]
        """
        x = self.net(tau)
        g = torch.sigmoid(x)  # (0,1)
        return self.gamma_min + (self.gamma_max - self.gamma_min) * g


# =========================
# 2) Temporal-Safe Adapter
# =========================
class TemporalSafeAdapter(nn.Module):
    """
    åœ¨ T5 encoder è¾“å‡ºç©ºé—´åšä½ç§©ç“¶é¢ˆæ˜ å°„ï¼Œå¹¶ç”¨æ—¶é—´é—¨æ§æ§åˆ¶å¼ºåº¦ï¼š
        H_safe = H + gate(Ï„) * Up(GELU(Down(LN(H))))
    """
    def __init__(self, hidden_size: int, rank: int = 256, init_gate: float = 0.5,
                 gamma_min: float = 0.5, gamma_max: float = 1.5):
        super().__init__()
        self.ln = nn.LayerNorm(hidden_size)
        self.down = nn.Linear(hidden_size, rank, bias=False)
        self.up = nn.Linear(rank, hidden_size, bias=False)
        self.act = nn.GELU()

        # learnable base gateï¼ˆæ ‡é‡åŸºçº¿ï¼Œå¯å­¦ä¹ ï¼‰
        self.base_gate = nn.Parameter(torch.tensor(init_gate, dtype=torch.float32))

        # temporal controller
        self.controller = TemporalController(hidden=16, gamma_min=gamma_min, gamma_max=gamma_max)

        # åˆå§‹åŒ–
        nn.init.kaiming_uniform_(self.down.weight, a=math.sqrt(5))
        nn.init.zeros_(self.up.weight)

    def forward(self, hidden_states: torch.Tensor, tau: float = 0.0):
        """
        hidden_states: [B, L, D]
        tau: å½’ä¸€åŒ–æ—¶é—´æ ‡é‡ (0~1)ï¼Œç”¨äºäº§ç”Ÿé—¨æ§
        """
        x = self.ln(hidden_states)
        delta = self.up(self.act(self.down(x)))

        tau_tensor = torch.tensor([[tau]], device=hidden_states.device, dtype=hidden_states.dtype)
        gamma = self.controller(tau_tensor)  # [1,1]
        gate = self.base_gate * gamma  # å¹¿æ’­åˆ° [B,L,D] è‡ªåŠ¨å®Œæˆ

        return hidden_states + gate * delta


# =========================
# 3) åŒ…è£… T5 Encoderï¼ˆå¯è¢« pipeline ç›´æ¥ä½¿ç”¨ï¼‰
# =========================
class WrappedTextEncoder(nn.Module):
    """
    åŒ…è£…åŸå§‹ T5Encoderï¼Œä½¿å…¶ forward æ—¶é€šè¿‡ TemporalSafeAdapter åšæ—¶åºé—¨æ§æ˜ å°„ã€‚
    é€šè¿‡ set_tau() æ¥å£æ§åˆ¶å½“å‰ Ï„ï¼›è‹¥ä¸æ˜¾å¼è®¾ç½®åˆ™ Ï„=0.0ã€‚
    """
    def __init__(self, t5_encoder: nn.Module, adapter: TemporalSafeAdapter):
        super().__init__()
        self.t5 = t5_encoder
        self.adapter = adapter

        for p in self.t5.parameters():
            p.requires_grad_(False)  # å†»ç»“åº•åº§

        self._current_tau = 0.0  # å¤–éƒ¨å¯ä¿®æ”¹

    # diffusers pipeline ä¼šç”¨åˆ°
    @property
    def dtype(self):
        try:
            return next(self.parameters()).dtype
        except StopIteration:
            return torch.float32

    @property
    def device(self):
        try:
            return next(self.parameters()).device
        except StopIteration:
            return torch.device("cpu")

    def set_tau(self, tau: float):
        self._current_tau = float(max(0.0, min(1.0, tau)))

    def forward(self, input_ids=None, attention_mask=None, inputs_embeds=None, **kwargs):
        outputs = self.t5(
            input_ids=input_ids,
            attention_mask=attention_mask,
            inputs_embeds=inputs_embeds,
            output_hidden_states=False,
            return_dict=True,
        )
        hs = outputs.last_hidden_state  # [B, L, D]
        hs_safe = self.adapter(hs, tau=self._current_tau)
        outputs.last_hidden_state = hs_safe
        return outputs


# =========================
# 4) æ•°æ®é›†
# =========================
class PairDataset(Dataset):
    """
    CSV åˆ—åŒ…å«: prompt, rewritten_prompt, benign_prompt
    """
    def __init__(self, csv_path: str):
        self.data = pd.read_csv(csv_path)
        self.malicious = self.data['prompt'].astype(str).tolist()
        self.rewritten = self.data['rewritten_prompt'].astype(str).tolist()
        self.benign = self.data['benign_prompt'].astype(str).tolist() if 'benign_prompt' in self.data.columns else []
        if not self.benign:
            self.benign = self.rewritten  # æ²¡æœ‰å°±ç”¨ rewritten ä»£æ›¿
        print(f"Loaded {len(self.malicious)} samples from {csv_path}")

    def __len__(self):
        return len(self.malicious)

    def __getitem__(self, i):
        return {
            "malicious": self.malicious[i],
            "rewritten": self.rewritten[i],
            "benign": self.benign[i],
        }


def collate_fn(batch: List[Dict[str, str]]):
    return {
        "malicious": [b["malicious"] for b in batch],
        "rewritten": [b["rewritten"] for b in batch],
        "benign": [b["benign"] for b in batch],
    }


# =========================
# 5) Trainer
# =========================
class TemporalSafeAdapterTrainer:
    """
    è®­ç»ƒä»…æ›´æ–° Adapterï¼ˆå« controllerï¼‰ï¼Œå†»ç»“ T5 baseã€‚
    æŸå¤±ï¼š
      - Align: æ¶æ„ vs é‡å†™ï¼ˆé€Ï„ï¼‰
      - Temporal: ç›¸é‚» Ï„ åµŒå…¥å¹³æ»‘
      - Benign-keep: è‰¯æ€§ä¿æŒï¼ˆé€‚é…å™¨å‰åï¼‰
    """
    def __init__(self, model_path, hidden_size=4096, rank=256, lr=5e-4, device="cuda",
                 lambda_temporal=0.5, lambda_benign=0.1, T_steps=8,
                 gamma_min=0.5, gamma_max=1.5):
        self.device = device
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, subfolder="tokenizer")
        base = T5EncoderModel.from_pretrained(model_path, subfolder="text_encoder").to(device)

        adapter = TemporalSafeAdapter(hidden_size=hidden_size, rank=rank,
                                      init_gate=0.5, gamma_min=gamma_min, gamma_max=gamma_max).to(device)
        self.model = WrappedTextEncoder(base, adapter).to(device)

        # ä»…è®­ç»ƒ adapterï¼ˆåŒ…æ‹¬æ—¶é—´ controllerï¼‰
        for p in self.model.parameters():
            p.requires_grad_(False)
        for p in self.model.adapter.parameters():
            p.requires_grad_(True)

        self.opt = torch.optim.AdamW(
            filter(lambda p: p.requires_grad, self.model.parameters()),
            lr=lr
        )
        self.lambda_temporal = lambda_temporal
        self.lambda_benign = lambda_benign
        self.T_steps = T_steps

        # å†»ç»“çš„ base encoderï¼ˆåš benign ä¿æŒçš„å‚è€ƒï¼‰
        self.base_encoder = T5EncoderModel.from_pretrained(model_path, subfolder="text_encoder").to(device)
        self.base_encoder.eval().requires_grad_(False)
        self.base_tokenizer = AutoTokenizer.from_pretrained(model_path, subfolder="tokenizer")

    @torch.no_grad()
    def _base_encode(self, texts: List[str]) -> torch.Tensor:
        toks = self.base_tokenizer(texts, padding=True, truncation=True, max_length=64, return_tensors="pt").to(self.device)
        out = self.base_encoder(**toks).last_hidden_state  # [B,L,D]
        return out.mean(dim=1)  # [B,D]

    def _encode_with_tau(self, texts: List[str], tau: float) -> torch.Tensor:
        batch = self.tokenizer(texts, padding=True, truncation=True, max_length=64, return_tensors="pt").to(self.device)
        self.model.set_tau(tau)
        # T5 å†»ç»“ï¼Œadapter å‚ä¸æ¢¯åº¦
        with torch.no_grad():
            hs = self.model.t5(**batch).last_hidden_state  # [B,L,D]
        hs_safe = self.model.adapter(hs, tau=tau)  # [B,L,D]
        return hs_safe.mean(dim=1)  # [B,D]

    def step(self, batch: Dict[str, List[str]]) -> Dict[str, float]:
        self.opt.zero_grad()
        mal, rew, ben = batch["malicious"], batch["rewritten"], batch["benign"]

        # é€ Ï„ é‡‡æ ·ï¼ˆç­‰è·æˆ–éšæœºï¼‰ï¼Œè¿™é‡Œç”¨ç­‰è·
        taus = [t / max(self.T_steps - 1, 1) for t in range(self.T_steps)]

        # ç¼–ç åºåˆ—
        mal_seq, rew_seq, ben_seq = [], [], []
        for tau in taus:
            mal_seq.append(self._encode_with_tau(mal, tau))  # [B,D]
            rew_seq.append(self._encode_with_tau(rew, tau))
            ben_seq.append(self._encode_with_tau(ben, tau))

        mal_seq = torch.stack(mal_seq, dim=0)  # [T,B,D]
        rew_seq = torch.stack(rew_seq, dim=0)  # [T,B,D]
        ben_seq = torch.stack(ben_seq, dim=0)  # [T,B,D]

        # 1) å¯¹é½æŸå¤±ï¼ˆé€ Ï„ï¼‰
        align = F.mse_loss(mal_seq, rew_seq)

        # 2) æ—¶åºå¹³æ»‘ï¼ˆç›¸é‚» Ï„ï¼‰
        temporal = F.mse_loss(mal_seq[1:], mal_seq[:-1]) + F.mse_loss(rew_seq[1:], rew_seq[:-1])

        # 3) è‰¯æ€§ä¿æŒï¼ˆä¸ base encoder çš„å¥å‘é‡å¯¹é½ï¼‰
        with torch.no_grad():
            ben_base = self._base_encode(ben)  # [B,D]
            ben_base = ben_base.unsqueeze(0).expand(self.T_steps, -1, -1)
        benign_keep = F.mse_loss(ben_seq, ben_base)

        loss = align + self.lambda_temporal * temporal + self.lambda_benign * benign_keep
        loss.backward()
        self.opt.step()

        return {
            "loss": float(loss.item()),
            "align": float(align.item()),
            "temporal": float(temporal.item()),
            "benign_keep": float(benign_keep.item()),
        }

    def save_adapter(self, path: str):
        os.makedirs(os.path.dirname(path), exist_ok=True)
        torch.save(self.model.adapter.state_dict(), path)
        print(f"âœ… TemporalSafeAdapter saved to {path}")

    def load_adapter(self, path: str):
        sd = torch.load(path, map_location="cpu")
        self.model.adapter.load_state_dict(sd, strict=True)
        print(f"âœ… TemporalSafeAdapter loaded from {path}")


# =========================
# 6) Pipeline æ³¨å…¥/è¯„ä¼°ï¼ˆåˆ†æ®µé—¨æ§ï¼‰
# =========================
def inject_temporal_adapter(pipe: CogVideoXPipeline, adapter_path: str,
                            hidden_size=4096, rank=256,
                            gamma_min=0.5, gamma_max=1.5):
    # æ„å»º + åŠ è½½ adapter
    adapter = TemporalSafeAdapter(hidden_size=hidden_size, rank=rank,
                                  init_gate=0.5, gamma_min=gamma_min, gamma_max=gamma_max)
    sd = torch.load(adapter_path, map_location="cpu")
    adapter.load_state_dict(sd, strict=True)

    # åŒ…è£… text_encoder
    wrapped = WrappedTextEncoder(pipe.text_encoder, adapter)
    wrapped = wrapped.to(device=pipe.device, dtype=getattr(pipe.text_encoder, "dtype", torch.float16))

    # æ³¨å†Œå› pipeline
    pipe.text_encoder = wrapped
    components = pipe.components.copy()
    components["text_encoder"] = pipe.text_encoder
    pipe.register_modules(**components)
    pipe.to(pipe.device)

    print("âœ… TemporalSafeAdapter injected into pipeline.text_encoder")
    return pipe


def generate_video_with_segments(
    pipe: CogVideoXPipeline,
    prompt: str,
    total_frames: int,
    num_segments: int,
    height: int,
    width: int,
    steps: int,
    guidance: float,
    fps: int,
    out_path: str,
    controller_schedule: str = "learned",  # "learned" | "linear" | "const"
):
    """
    åˆ†æ®µæ¨ç†ï¼šæ¯æ®µè®¾ç½®ä¸åŒ tauï¼Œç”Ÿæˆå­è§†é¢‘ï¼ˆè‹¥å¹²å¸§ï¼‰ï¼Œæœ€ååˆå¹¶æˆä¸€ä¸ªè§†é¢‘ã€‚
    å…¼å®¹ pipeline å†…éƒ¨â€œå•æ¬¡æ–‡æœ¬ç¼–ç â€çš„å‡è®¾ã€‚
    """
    assert hasattr(pipe.text_encoder, "set_tau"), "text_encoder must be WrappedTextEncoder with set_tau()."

    frames_all = []
    frames_per_seg = max(1, total_frames // num_segments)
    rest = total_frames - frames_per_seg * num_segments

    for seg in range(num_segments):
        seg_frames = frames_per_seg + (1 if seg < rest else 0)

        # è®¡ç®—è¯¥æ®µçš„ tauï¼ˆ0~1ï¼‰ã€‚è¿™é‡Œæä¾›ä¸‰ç§ç®€å•ç­–ç•¥ï¼š
        if controller_schedule == "linear":
            tau = seg / max(num_segments - 1, 1)
        elif controller_schedule == "const":
            tau = 0.5
        else:  # "learned"ï¼šæ²¿ç”¨è®­ç»ƒæ—¶çš„ä¹ æƒ¯ï¼Œç”¨æ®µç´¢å¼•æ˜ å°„åˆ° [0,1]ï¼›è‹¥ adapter å†…æœ‰æ›´å¤æ‚ controller äº¦å¯æ‹“å±•
            tau = seg / max(num_segments - 1, 1)

        pipe.text_encoder.set_tau(tau)

        out = pipe(
            prompt=prompt,
            num_frames=seg_frames,
            height=height,
            width=width,
            num_inference_steps=steps,
            guidance_scale=guidance,
        ).frames[0]  # list of PIL images
        frames_all.extend(out)

    export_to_video(frames_all, out_path, fps=fps)
    print(f"ğŸ¬ saved merged video to {out_path}")


# =========================
# 7) è®­ç»ƒ / è¯„ä¼°å…¥å£
# =========================
def train_adapter(args):
    trainer = TemporalSafeAdapterTrainer(
        model_path=args.model_path,
        hidden_size=args.hidden_size,
        rank=args.rank,
        lr=args.lr,
        device=args.device,
        lambda_temporal=args.lambda_temporal,
        lambda_benign=args.lambda_benign,
        T_steps=args.T_steps,
        gamma_min=args.gamma_min,
        gamma_max=args.gamma_max,
    )

    ds = PairDataset(args.trainset_path)
    loader = DataLoader(ds, batch_size=args.batch_size, shuffle=True, collate_fn=collate_fn)

    for epoch in range(args.num_epochs):
        pbar = tqdm(loader, desc=f"Epoch {epoch+1}/{args.num_epochs}", dynamic_ncols=True)
        for batch in pbar:
            logs = trainer.step(batch)
            pbar.set_postfix({k: f"{v:.3f}" for k, v in logs.items()})

        if args.save_every and (epoch + 1) % args.save_every == 0:
            ckpt_path = args.adapter_path.replace(".pt", f"_epoch{epoch+1}.pt")
            trainer.save_adapter(ckpt_path)

    trainer.save_adapter(args.adapter_path)


def eval_adapter(args):
    # baseline pipelineï¼ˆæœªæ³¨å…¥ï¼‰
    pipe_raw = CogVideoXPipeline.from_pretrained(args.model_path, torch_dtype=torch.float16)
    pipe_raw.scheduler = CogVideoXDPMScheduler.from_config(pipe_raw.scheduler.config, timestep_spacing="trailing")
    pipe_raw.to(args.device)
    pipe_raw.vae.enable_slicing()
    pipe_raw.vae.enable_tiling()

    # æ³¨å…¥ adapter çš„ pipeline
    pipe_safe = CogVideoXPipeline.from_pretrained(args.model_path, torch_dtype=torch.float16)
    pipe_safe.scheduler = CogVideoXDPMScheduler.from_config(pipe_safe.scheduler.config, timestep_spacing="trailing")
    pipe_safe.to(args.device)
    pipe_safe.vae.enable_slicing()
    pipe_safe.vae.enable_tiling()
    pipe_safe = inject_temporal_adapter(
        pipe_safe, args.adapter_path, hidden_size=args.hidden_size, rank=args.rank,
        gamma_min=args.gamma_min, gamma_max=args.gamma_max
    )

    os.makedirs(args.output_dir, exist_ok=True)

    data = pd.read_csv(args.testset_path)
    prompts = data["prompt"].astype(str).tolist()

    for i, prompt in enumerate(prompts):
        # raw
        video_raw = pipe_raw(
            prompt=prompt,
            num_frames=args.num_frames,
            height=args.height,
            width=args.width,
            num_inference_steps=args.num_inference_steps,
            guidance_scale=args.guidance_scale,
        ).frames[0]
        path_raw = f"{args.output_dir}/tempSafe_{i:03d}_raw.mp4"
        export_to_video(video_raw, path_raw, fps=args.fps)
        print(f"âœ… RAW saved to {path_raw}")

        # safeï¼ˆåˆ†æ®µé—¨æ§ï¼‰
        path_safe = f"{args.output_dir}/tempSafe_{i:03d}_safe.mp4"
        generate_video_with_segments(
            pipe=pipe_safe,
            prompt=prompt,
            total_frames=args.num_frames,
            num_segments=args.num_segments,
            height=args.height,
            width=args.width,
            steps=args.num_inference_steps,
            guidance=args.guidance_scale,
            fps=args.fps,
            out_path=path_safe,
            controller_schedule=args.controller_schedule,  # "learned"|"linear"|"const"
        )


def build_args():
    cfg = {
        "mode": "eval",  # "train" or "eval"
        "model_path": "/home/beihang/jzl/models/zai-org/CogVideoX-5b",
        "hidden_size": 4096,
        "rank": 256,
        "gamma_min": 0.5,
        "gamma_max": 1.5,
        "lr": 5e-4,
        "device": "cuda",

        # train
        "num_epochs": 10,
        "batch_size": 8,
        "T_steps": 8,                # æ¯ä¸ª batch çš„ Ï„ é‡‡æ ·æ­¥æ•°
        "lambda_temporal": 0.5,
        "lambda_benign": 0.1,
        "save_every": 2,
        "trainset_path": "datasets/train/2.csv",
        "adapter_path": "checkpoints/temporal_safe_adapter.pt",

        # eval / gen
        "testset_path": "datasets/test/demo.csv",
        "output_dir": "out/temporal_demo",
        "num_frames": 49,
        "height": 480,
        "width": 720,
        "num_inference_steps": 28,
        "guidance_scale": 6.0,
        "fps": 24,

        # åˆ†æ®µé—¨æ§è®¾ç½®
        "num_segments": 7,                 # æŠŠè§†é¢‘åˆ†æˆ 7 æ®µï¼Œæ¯æ®µå•ç‹¬è®¾ tau
        "controller_schedule": "learned",  # "learned"|"linear"|"const"
    }
    return argparse.Namespace(**cfg)


if __name__ == "__main__":
    args = build_args()
    if args.mode == "train":
        train_adapter(args)
    else:
        eval_adapter(args)
