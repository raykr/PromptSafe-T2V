import torch
import torch.nn as nn
import math


class SafeAdapter(nn.Module):
    """
    å†»ç»“ T5 è¾“å‡ºåï¼Œåšä¸€ä¸ªå°ç“¶é¢ˆ + æ®‹å·®çš„å®‰å…¨æ˜ å°„å±‚
    H_safe = H + gate * scale * MLP(LN(H))
    å…¶ä¸­ scale ç”±å¤–éƒ¨åŠ¨æ€æ§åˆ¶ï¼ˆä¾‹å¦‚æ¥è‡ª prompt åˆ†ç±»å™¨ï¼‰
    """

    def __init__(self, hidden_size: int, rank: int = 256, init_gate: float = 0.5):
        super().__init__()
        self.ln = nn.LayerNorm(hidden_size)
        self.down = nn.Linear(hidden_size, rank, bias=False)
        self.up = nn.Linear(rank, hidden_size, bias=False)
        self.act = nn.GELU()
        # å¯å­¦ä¹  base gate
        self.gate = nn.Parameter(torch.tensor(init_gate))

        # å°åˆå§‹åŒ–ï¼Œé¿å…ä¸€å¼€å§‹ç ´ååˆ†å¸ƒ
        nn.init.kaiming_uniform_(self.down.weight, a=math.sqrt(5))
        nn.init.zeros_(self.up.weight)

    def forward(self, hidden_states: torch.Tensor, scale: torch.Tensor | float = 1.0):
        """
        scale: åŠ¨æ€é˜²å¾¡å¼ºåº¦ç³»æ•°
          - å¯ä»¥æ˜¯æ ‡é‡ float
          - ä¹Ÿå¯ä»¥æ˜¯ [B] æˆ– [B, 1, 1] çš„ Tensorï¼ˆä¼šè‡ªåŠ¨å¹¿æ’­ï¼‰
        """
        x = self.ln(hidden_states)
        delta = self.up(self.act(self.down(x)))  # [B, L, D]

        gate = self.gate
        if not torch.is_tensor(scale):
            scale = torch.tensor(scale, device=hidden_states.device, dtype=hidden_states.dtype)
        # scale å½¢çŠ¶è°ƒæ•´ä¸º [B, 1, 1] æˆ– [1, 1, 1]ï¼Œæ–¹ä¾¿å¹¿æ’­
        while scale.dim() < hidden_states.dim():
            scale = scale.unsqueeze(-1)

        eff_gate = gate * scale  # [B,1,1] or [1,1,1]

        return hidden_states + eff_gate * delta



class WrappedTextEncoder(nn.Module):
    def __init__(self, t5_encoder: nn.Module, adapter: SafeAdapter):
        super().__init__()
        self.t5 = t5_encoder
        for p in self.t5.parameters():
            p.requires_grad_(False)
        self.adapter = adapter

        # é»˜è®¤åŠ¨æ€ scale = 1.0ï¼Œå¯ä»¥åœ¨æ¨ç†å‰ç”±å¤–éƒ¨ä¿®æ”¹
        self.adapter_scale = 1.0

    # ğŸ”§ å¯¹å¤–æš´éœ²ä¸€ä¸ªè®¾ç½®æ¥å£ï¼Œæ–¹ä¾¿åœ¨ç”Ÿæˆå‰æ ¹æ® prompt åˆ†ç±»ç»“æœåŠ¨æ€è°ƒèŠ‚
    def set_adapter_scale(self, scale: torch.Tensor | float):
        """
        scale å¯ä»¥æ˜¯:
          - float æ ‡é‡ï¼šå¯¹å½“å‰ batch ä½¿ç”¨ç»Ÿä¸€é˜²å¾¡å¼ºåº¦
          - [B] Tensorï¼šå¯¹ batch å†…æ¯ä¸ªæ ·æœ¬ç”¨ä¸åŒå¼ºåº¦
        """
        self.adapter_scale = scale

    # diffusers çš„ pipeline.to() ä¼šè¯»å–è¿™äº›å±æ€§
    @property
    def dtype(self):
        try:
            return next(self.parameters()).dtype
        except StopIteration:
            return torch.float32

    @property
    def device(self):
        try:
            return next(self.parameters()).device
        except StopIteration:
            return torch.device("cpu")

    def forward(self, input_ids=None, attention_mask=None, inputs_embeds=None, **kwargs):
        outputs = self.t5(
            input_ids=input_ids,
            attention_mask=attention_mask,
            inputs_embeds=inputs_embeds,
            output_hidden_states=False,
            return_dict=True,
        )
        hs = outputs.last_hidden_state  # [B, L, D]
        # å°†å½“å‰å¯¹è±¡çš„ adapter_scale ä¼ ç»™ adapter
        hs_safe = self.adapter(hs, scale=self.adapter_scale)
        outputs.last_hidden_state = hs_safe
        return outputs


class AdapterRouter(nn.Module):
    """
    å¤š adapter è·¯ç”±å™¨ï¼š
      - å†…éƒ¨æœ‰è‹¥å¹²ä¸ª SafeAdapterï¼Œæ¯ä¸ªå¯¹åº”ä¸€ä¸ªæœ‰å®³ç±»åˆ«
      - æ¨ç†æ—¶é€šè¿‡ set_route(category, scale) é€‰æ‹©ä¸€ä¸ª adapter + é˜²å¾¡å¼ºåº¦
    """
    def __init__(self, adapters: dict[str, SafeAdapter]):
        super().__init__()
        # {'sexual': adapter_sexual, 'violent': adapter_violent, ...}
        self.adapters = nn.ModuleDict(adapters)
        self.current_category: str | None = None
        self.current_scale: torch.Tensor | float = 1.0

    def set_route(self, category: str | None, scale: torch.Tensor | float = 1.0):
        """
        è®¾ç½®å½“å‰ä½¿ç”¨å“ªä¸€ä¸ª adapterï¼Œä»¥åŠé˜²å¾¡å¼ºåº¦ scale
        category:
          - ä¸º None æ—¶è¡¨ç¤ºä¸ä½¿ç”¨ä»»ä½• adapterï¼ˆå®Œå…¨å…³é—­é˜²å¾¡ï¼‰
          - ä¸ºæŸä¸ª key æ—¶ä½¿ç”¨å¯¹åº”çš„ adapter
        scale:
          - float æˆ–è€… [B] Tensor
        """
        self.current_category = category
        self.current_scale = scale

    def forward(self, hidden_states: torch.Tensor):
        if self.current_category is None:
            # ä¸è¿›è¡Œä»»ä½•é˜²å¾¡
            return hidden_states
        if self.current_category not in self.adapters:
            # é˜²å¾¡ç±»åˆ«æœªæ³¨å†Œï¼Œé€€åŒ–ä¸º no-op
            return hidden_states

        adapter = self.adapters[self.current_category]
        return adapter(hidden_states, scale=self.current_scale)


class WrappedTextEncoderRouter(nn.Module):
    def __init__(self, t5_encoder: nn.Module, router: AdapterRouter):
        super().__init__()
        self.t5 = t5_encoder
        for p in self.t5.parameters():
            p.requires_grad_(False)
        self.router = router

    # å¯¹å¤–æš´éœ²è®¾ç½®è·¯ç”±çš„æ¥å£
    def set_adapter_route(self, category: str | None, scale: torch.Tensor | float = 1.0):
        self.router.set_route(category, scale)

    @property
    def dtype(self):
        try:
            return next(self.parameters()).dtype
        except StopIteration:
            return torch.float32

    @property
    def device(self):
        try:
            return next(self.parameters()).device
        except StopIteration:
            return torch.device("cpu")

    def forward(self, input_ids=None, attention_mask=None, inputs_embeds=None, **kwargs):
        outputs = self.t5(
            input_ids=input_ids,
            attention_mask=attention_mask,
            inputs_embeds=inputs_embeds,
            output_hidden_states=False,
            return_dict=True,
        )
        hs = outputs.last_hidden_state  # [B,L,D]
        hs_safe = self.router(hs)       # æ ¹æ®å½“å‰ route é€‰æ‹©ä¸€ä¸ª adapter + scale
        outputs.last_hidden_state = hs_safe
        return outputs
