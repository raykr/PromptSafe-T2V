"""
T2Vå®‰å…¨Adapteré˜²å¾¡æ•ˆæœè¯„ä¼° - æ ¸å¿ƒæŒ‡æ ‡å®ç°
æ”¯æŒä¸‰ä¸ªæŒ‡æ ‡ï¼šCLIP Score, å…‰æµä¸€è‡´æ€§, LPIPS

ä¾èµ–ï¼š
    pip install torch torchvision transformers clip-score torchmetrics
    pip install av  # ç”¨äºè§†é¢‘è¯»å–
"""

import torch
import torch.nn.functional as F
import numpy as np
from typing import Union, Tuple, List, Optional
import cv2
from pathlib import Path
import warnings

# ============================================================================
# 1. CLIP SCORE å®ç°
# ============================================================================

class CLIPScoreCalculator:
    """
    CLIP Scoreè®¡ç®—å™¨ - è¯„ä¼°æ–‡æœ¬-è§†é¢‘å¯¹é½åº¦
    
    å…¬å¼: CLIPScore = max(100 * cos(E_video, E_text), 0)
    èŒƒå›´: [0, 100]ï¼Œåˆ†æ•°è¶Šé«˜è¶Šå¥½
    """
    
    def __init__(self, model_name: str = "openai/clip-vit-base-patch32", device: str = "cuda"):
        """
        åˆå§‹åŒ–CLIPæ¨¡å‹
        
        Args:
            model_name: CLIPæ¨¡å‹åç§°ï¼Œå¯é€‰ï¼š
                - "openai/clip-vit-base-patch32" (æ¨èï¼Œå¿«é€Ÿ)
                - "openai/clip-vit-base-patch16" (æ›´ç²¾å‡†)
                - "openai/clip-vit-large-patch14" (æœ€ç²¾å‡†ï¼Œæ…¢)
            device: è®¡ç®—è®¾å¤‡ "cuda" æˆ– "cpu"
        """
        from transformers import CLIPProcessor, CLIPModel
        from PIL import Image
        
        self.device = device
        self.model = CLIPModel.from_pretrained(model_name).to(device).eval()
        self.processor = CLIPProcessor.from_pretrained(model_name)
        self.model_name = model_name
        
        print(f"âœ“ CLIP Scoreæ¨¡å‹åŠ è½½æˆåŠŸ: {model_name}")
    
    @torch.no_grad()
    def compute_clip_score(
        self, 
        video_frames: Union[torch.Tensor, np.ndarray, List[np.ndarray]],
        text_prompt: str,
        sample_frames: Optional[int] = None,
        return_per_frame: bool = False
    ) -> Union[float, Tuple[float, List[float]]]:
        """
        è®¡ç®—CLIP Score
        
        Args:
            video_frames: è§†é¢‘å¸§æ•°æ®ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼š
                - torch.Tensor: (T, C, H, W) æˆ– (T, H, W, C) çš„å¼ é‡ï¼ŒèŒƒå›´[0,255]æˆ–[0,1]
                - np.ndarray: åŒä¸Š
                - List[np.ndarray]: å¸§åˆ—è¡¨ï¼Œæ¯å¸§ (H, W, 3) RGBæ ¼å¼
            text_prompt: æ–‡æœ¬æç¤º
            sample_frames: é‡‡æ ·å¸§æ•°ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨æ‰€æœ‰å¸§ï¼ˆå¯¹é•¿è§†é¢‘ä½¿ç”¨æ­¤å‚æ•°é™ä½è®¡ç®—æˆæœ¬ï¼‰
            return_per_frame: æ˜¯å¦è¿”å›æ¯å¸§çš„åˆ†æ•°
        
        Returns:
            clip_score: å¹³å‡CLIP Score (0-100)
            per_frame_scores: å¦‚æœreturn_per_frame=Trueï¼Œè¿”å›(avg_score, å•å¸§åˆ†æ•°åˆ—è¡¨)
        
        Example:
            >>> calculator = CLIPScoreCalculator()
            >>> video = torch.randn(30, 3, 512, 512) * 255  # 30å¸§è§†é¢‘
            >>> score = calculator.compute_clip_score(video, "a cat running")
            >>> print(f"CLIP Score: {score:.2f}")
        """
        from PIL import Image
        
        # 1. è½¬æ¢è§†é¢‘æ ¼å¼ä¸ºå¸§åˆ—è¡¨
        frames = self._convert_to_frames(video_frames)
        
        # 2. é‡‡æ ·å¸§
        if sample_frames is not None and len(frames) > sample_frames:
            indices = np.linspace(0, len(frames) - 1, sample_frames, dtype=int)
            frames = [frames[i] for i in indices]
            print(f"  é‡‡æ ·{sample_frames}å¸§ç”¨äºCLIP Scoreè®¡ç®—")
        
        # 3. å¯¹æ–‡æœ¬ç¼–ç 
        text_inputs = self.processor(text=text_prompt, return_tensors="pt").to(self.device)
        text_embedding = self.model.get_text_features(**text_inputs)
        text_embedding = F.normalize(text_embedding, p=2, dim=-1)  # [1, 512]
        
        # 4. å¯¹æ¯å¸§ç¼–ç å¹¶è®¡ç®—åˆ†æ•°
        per_frame_scores = []
        
        for frame in frames:
            # è½¬ä¸ºPIL Image
            if isinstance(frame, np.ndarray):
                if frame.dtype != np.uint8:
                    frame = (frame * 255).astype(np.uint8)
                frame = Image.fromarray(frame)
            
            # å¤„ç†å›¾åƒ
            image_inputs = self.processor(images=frame, return_tensors="pt").to(self.device)
            image_embedding = self.model.get_image_features(**image_inputs)
            image_embedding = F.normalize(image_embedding, p=2, dim=-1)  # [1, 512]
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = torch.nn.functional.cosine_similarity(
                text_embedding, 
                image_embedding
            )
            clip_score = max(100 * similarity.item(), 0)
            per_frame_scores.append(clip_score)
        
        avg_score = np.mean(per_frame_scores)
        
        if return_per_frame:
            return avg_score, per_frame_scores
        else:
            return avg_score
    
    def _convert_to_frames(self, video_frames) -> List[np.ndarray]:
        """å°†å„ç§æ ¼å¼è½¬æ¢ä¸ºå¸§åˆ—è¡¨ (H, W, 3) RGB uint8"""
        
        if isinstance(video_frames, list):
            # åˆ—è¡¨æ ¼å¼
            frames = []
            for frame in video_frames:
                if isinstance(frame, torch.Tensor):
                    frame = frame.cpu().numpy()
                if frame.ndim == 3 and frame.shape[0] == 3:  # (C, H, W)
                    frame = np.transpose(frame, (1, 2, 0))
                if frame.dtype != np.uint8:
                    if frame.max() <= 1.0:
                        frame = (frame * 255).astype(np.uint8)
                    else:
                        frame = frame.astype(np.uint8)
                frames.append(frame)
            return frames
        
        elif isinstance(video_frames, torch.Tensor):
            video_frames = video_frames.cpu().numpy()
        
        # numpy arrayå¤„ç†
        if video_frames.ndim == 4:
            if video_frames.shape[1] == 3:  # (T, C, H, W)
                video_frames = np.transpose(video_frames, (0, 2, 3, 1))
            # ç°åœ¨æ˜¯ (T, H, W, 3)
        
        frames = []
        for i in range(video_frames.shape[0]):
            frame = video_frames[i]
            if frame.dtype != np.uint8:
                if frame.max() <= 1.0:
                    frame = (frame * 255).astype(np.uint8)
                else:
                    frame = frame.astype(np.uint8)
            frames.append(frame)
        
        return frames


# ============================================================================
# 2. å…‰æµä¸€è‡´æ€§å®ç°ï¼ˆRAFTï¼‰
# ============================================================================

class OpticalFlowConsistency:
    """
    åŸºäºRAFTçš„å…‰æµä¸€è‡´æ€§è¯„ä¼°
    è¡¡é‡ç›¸é‚»å¸§é—´çš„è¿åŠ¨å¹³æ»‘ç¨‹åº¦ - é˜²å¾¡å¯èƒ½å¼•å…¥å¸§é—ªçƒ
    
    å…¬å¼: è®¡ç®—æ‰€æœ‰ç›¸é‚»å¸§å¯¹çš„å…‰æµå¹…åº¦ç»Ÿè®¡é‡
    """
    
    def __init__(self, device: str = "cuda"):
        """
        åˆå§‹åŒ–RAFTå…‰æµæ¨¡å‹
        
        Args:
            device: è®¡ç®—è®¾å¤‡
        """
        try:
            from torchvision.models.optical_flow import raft_large
            self.device = device
            self.raft = raft_large(pretrained=True).to(device).eval()
            print("âœ“ RAFTå…‰æµæ¨¡å‹åŠ è½½æˆåŠŸ")
        except ImportError:
            print("âš  éœ€è¦ torchvision >= 0.13.0ï¼Œå°è¯•å®‰è£…ï¼špip install --upgrade torchvision")
            raise
    
    @torch.no_grad()
    def compute_optical_flow_consistency(
        self,
        video_frames: Union[torch.Tensor, np.ndarray, List[np.ndarray]],
        return_statistics: bool = True
    ) -> Union[float, dict]:
        """
        è®¡ç®—å…‰æµä¸€è‡´æ€§æŒ‡æ ‡
        
        Args:
            video_frames: è§†é¢‘å¸§ï¼Œæ ¼å¼åŒCLIP Score
            return_statistics: æ˜¯å¦è¿”å›è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
        
        Returns:
            consistency_score: ä¸€è‡´æ€§åˆ†æ•°ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šå¥½ï¼‰
            æˆ–åŒ…å«è¯¦ç»†ç»Ÿè®¡çš„å­—å…¸ï¼š
                - 'mean_magnitude': å¹³å‡å…‰æµå¹…åº¦
                - 'std_magnitude': å…‰æµå¹…åº¦æ ‡å‡†å·®
                - 'max_magnitude': æœ€å¤§å…‰æµå¹…åº¦
                - 'consistency_score': æœ€ç»ˆä¸€è‡´æ€§åˆ†æ•°
        
        Example:
            >>> flow_calc = OpticalFlowConsistency()
            >>> score = flow_calc.compute_optical_flow_consistency(video)
            >>> print(f"å…‰æµä¸€è‡´æ€§: {score:.3f}")
        """
        
        # 1. è½¬æ¢è§†é¢‘æ ¼å¼
        frames = self._convert_to_tensor(video_frames)  # (T, 3, H, W) in [0, 255]
        frames = frames.float() / 255.0  # å½’ä¸€åŒ–åˆ° [0, 1]
        
        # 2. RAFTè¦æ±‚çš„è¾“å…¥æ ¼å¼ - éœ€è¦åˆ›å»ºæ‰¹æ¬¡
        flow_magnitudes = []
        
        for i in range(len(frames) - 1):
            frame1 = frames[i:i+1].to(self.device)  # (1, 3, H, W)
            frame2 = frames[i+1:i+2].to(self.device)
            
            # RAFTè¾“å‡ºå…‰æµ (1, 2, H, W)
            flow_list = self.raft(frame1, frame2)
            flow = flow_list[-1]  # å–æœ€åä¸€å±‚çš„è¾“å‡º
            
            # è®¡ç®—å…‰æµå¹…åº¦
            magnitude = torch.norm(flow, p=2, dim=1, keepdim=True)  # (1, 1, H, W)
            flow_magnitudes.append(magnitude.cpu())
        
        # 3. ç»Ÿè®¡å…‰æµå¹…åº¦
        all_magnitudes = torch.cat(flow_magnitudes, dim=0)  # (T-1, 1, H, W)
        mean_mag = all_magnitudes.mean().item()
        std_mag = all_magnitudes.std().item()
        max_mag = all_magnitudes.max().item()
        
        # 4. ä¸€è‡´æ€§åˆ†æ•°ï¼šä½æ–¹å·® = é«˜ä¸€è‡´æ€§
        # ä½¿ç”¨ 1 - (std / mean) çš„å½¢å¼ï¼Œé™åˆ¶åœ¨[0,1]
        consistency_score = max(0, 1 - (std_mag / (mean_mag + 1e-6)))
        
        if return_statistics:
            return {
                'consistency_score': consistency_score,
                'mean_magnitude': mean_mag,
                'std_magnitude': std_mag,
                'max_magnitude': max_mag,
                'variation_coefficient': std_mag / (mean_mag + 1e-6)  # CVè¶Šå°è¶Šä¸€è‡´
            }
        else:
            return consistency_score
    
    def _convert_to_tensor(self, video_frames) -> torch.Tensor:
        """è½¬æ¢ä¸º (T, 3, H, W) æ ¼å¼çš„å¼ é‡"""
        
        if isinstance(video_frames, list):
            frames_list = []
            for frame in video_frames:
                if isinstance(frame, np.ndarray):
                    if frame.ndim == 3 and frame.shape[0] == 3:
                        frame = np.transpose(frame, (1, 2, 0))
                    frames_list.append(torch.from_numpy(frame))
                else:
                    frames_list.append(frame)
            # å‡è®¾ (H, W, 3) æ ¼å¼
            tensor = torch.stack(frames_list)
            if tensor.shape[-1] == 3:
                tensor = tensor.permute(0, 3, 1, 2)  # è½¬ä¸º (T, 3, H, W)
            return tensor
        
        elif isinstance(video_frames, np.ndarray):
            tensor = torch.from_numpy(video_frames)
            if tensor.shape[1] == 3:  # (T, 3, H, W)
                return tensor
            elif tensor.shape[-1] == 3:  # (T, H, W, 3)
                return tensor.permute(0, 3, 1, 2)
        
        elif isinstance(video_frames, torch.Tensor):
            if video_frames.shape[1] == 3:  # (T, 3, H, W)
                return video_frames
            elif video_frames.shape[-1] == 3:  # (T, H, W, 3)
                return video_frames.permute(0, 3, 1, 2)
        
        raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥æ ¼å¼: {type(video_frames)}")


# ============================================================================
# 3. LPIPS å®ç°ï¼ˆæ„ŸçŸ¥ç›¸ä¼¼æ€§ï¼‰
# ============================================================================

class LPIPSEvaluator:
    """
    LPIPS (Learned Perceptual Image Patch Similarity) è¯„ä¼°
    è¯„ä¼°ç›¸é‚»å¸§é—´çš„æ„ŸçŸ¥ç›¸ä¼¼åº¦ - é˜²å¾¡ä¸åº”é™ä½å¸§é—´çš„è¿è´¯æ€§
    
    è¯´æ˜: ä½LPIPS = é«˜ç›¸ä¼¼åº¦ = å¥½
    """
    
    def __init__(self, net_type: str = "alex", device: str = "cuda"):
        """
        åˆå§‹åŒ–LPIPS
        
        Args:
            net_type: éª¨å¹²ç½‘ç»œç±»å‹ - "alex" (å¿«), "vgg" (ä¸­ç­‰), "squeeze" (å°)
            device: è®¡ç®—è®¾å¤‡
        """
        try:
            from torchmetrics.image.lpip import LearnedPerceptualImagePatchSimilarity
            self.lpips = LearnedPerceptualImagePatchSimilarity(
                net_type=net_type, 
                reduction='none'
            ).to(device)
            self.device = device
            print(f"âœ“ LPIPSæ¨¡å‹åŠ è½½æˆåŠŸ (backbone: {net_type})")
        except ImportError:
            print("âš  éœ€è¦å®‰è£… torchmetricsï¼špip install torchmetrics")
            raise
    
    @torch.no_grad()
    def compute_lpips(
        self,
        video_frames: Union[torch.Tensor, np.ndarray, List[np.ndarray]],
        return_statistics: bool = True
    ) -> Union[float, dict]:
        """
        è®¡ç®—ç›¸é‚»å¸§é—´çš„LPIPS
        
        Args:
            video_frames: è§†é¢‘å¸§ï¼Œæ ¼å¼åŒä¸Š
            return_statistics: æ˜¯å¦è¿”å›è¯¦ç»†ç»Ÿè®¡
        
        Returns:
            avg_lpips: å¹³å‡LPIPS (0-1ï¼Œè¶Šä½è¶Šå¥½)
            æˆ–ç»Ÿè®¡å­—å…¸ï¼š
                - 'mean_lpips': å¹³å‡LPIPS
                - 'std_lpips': LPIPSæ ‡å‡†å·®
                - 'max_lpips': æœ€å¤§LPIPS
        
        Example:
            >>> evaluator = LPIPSEvaluator()
            >>> score = evaluator.compute_lpips(video)
            >>> print(f"å¹³å‡LPIPS: {score:.4f}")
        """
        
        # 1. è½¬æ¢è§†é¢‘æ ¼å¼ä¸º (T, 3, H, W) in [-1, 1]
        frames = self._convert_to_tensor(video_frames)
        
        # 2. è®¡ç®—ç›¸é‚»å¸§å¯¹çš„LPIPS
        lpips_scores = []
        
        for i in range(len(frames) - 1):
            frame1 = frames[i:i+1].to(self.device)
            frame2 = frames[i+1:i+2].to(self.device)
            
            # LPIPSæœŸæœ›è¾“å…¥åœ¨ [-1, 1]
            if frame1.max() > 1.0:
                frame1 = frame1 / 127.5 - 1.0
                frame2 = frame2 / 127.5 - 1.0
            
            score = self.lpips(frame1, frame2)
            lpips_scores.append(score.cpu().item())
        
        # 3. ç»Ÿè®¡
        mean_lpips = np.mean(lpips_scores)
        std_lpips = np.std(lpips_scores)
        max_lpips = np.max(lpips_scores)
        
        if return_statistics:
            return {
                'mean_lpips': mean_lpips,
                'std_lpips': std_lpips,
                'max_lpips': max_lpips,
                'per_frame_lpips': lpips_scores
            }
        else:
            return mean_lpips
    
    def _convert_to_tensor(self, video_frames) -> torch.Tensor:
        """è½¬æ¢ä¸º (T, 3, H, W) æ ¼å¼çš„å¼ é‡"""
        
        if isinstance(video_frames, list):
            frames_list = []
            for frame in video_frames:
                if isinstance(frame, np.ndarray):
                    if frame.ndim == 3 and frame.shape[2] == 3:  # (H, W, 3)
                        frame = np.transpose(frame, (2, 0, 1))
                    frames_list.append(torch.from_numpy(frame))
                else:
                    frames_list.append(frame)
            tensor = torch.stack(frames_list)
            return tensor.float()
        
        elif isinstance(video_frames, np.ndarray):
            tensor = torch.from_numpy(video_frames).float()
            if tensor.shape[1] == 3:  # (T, 3, H, W)
                return tensor
            elif tensor.shape[-1] == 3:  # (T, H, W, 3)
                return tensor.permute(0, 3, 1, 2)
        
        elif isinstance(video_frames, torch.Tensor):
            tensor = video_frames.float()
            if tensor.shape[1] == 3:
                return tensor
            elif tensor.shape[-1] == 3:
                return tensor.permute(0, 3, 1, 2)
        
        raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥æ ¼å¼: {type(video_frames)}")


# ============================================================================
# 4. ç»¼åˆè¯„ä¼°ç±»
# ============================================================================

class T2VVideoEvaluator:
    """
    T2Vç”Ÿæˆè§†é¢‘é˜²å¾¡æ•ˆæœç»¼åˆè¯„ä¼°å™¨
    """
    
    def __init__(self, device: str = "cuda"):
        """
        åˆå§‹åŒ–æ‰€æœ‰è¯„ä¼°å·¥å…·
        
        Args:
            device: è®¡ç®—è®¾å¤‡
        """
        self.device = device
        self.clip_calculator = CLIPScoreCalculator(device=device)
        self.flow_evaluator = OpticalFlowConsistency(device=device)
        self.lpips_evaluator = LPIPSEvaluator(device=device)
        print("\nâœ“ æ‰€æœ‰è¯„ä¼°å·¥å…·åˆå§‹åŒ–å®Œæˆ\n")
    
    def evaluate_defense_impact(
        self,
        video_original: Union[torch.Tensor, np.ndarray],
        video_defended: Union[torch.Tensor, np.ndarray],
        text_prompt: str,
        verbose: bool = True
    ) -> dict:
        """
        è¯„ä¼°é˜²å¾¡å¯¹è§†é¢‘è´¨é‡çš„å½±å“
        
        Args:
            video_original: é˜²å¾¡å‰çš„è§†é¢‘
            video_defended: é˜²å¾¡åçš„è§†é¢‘
            text_prompt: æ–‡æœ¬æç¤º
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        
        Returns:
            è¯„ä¼°ç»“æœå­—å…¸ï¼ŒåŒ…å«ä¸‰ä¸ªæŒ‡æ ‡çš„å¯¹æ¯”
        
        Example:
            >>> evaluator = T2VVideoEvaluator()
            >>> results = evaluator.evaluate_defense_impact(
            ...     video_original, video_defended, "a cat running"
            ... )
            >>> print(results)
        """
        
        results = {
            'text_prompt': text_prompt,
            'metrics': {}
        }
        
        # 1. CLIP Score
        clip_orig = self.clip_calculator.compute_clip_score(
            video_original, text_prompt
        )
        clip_def = self.clip_calculator.compute_clip_score(
            video_defended, text_prompt
        )
        
        results['metrics']['CLIP_Score'] = {
            'original': clip_orig,
            'defended': clip_def,
            'delta': clip_orig - clip_def,
            'delta_pct': (clip_orig - clip_def) / (clip_orig + 1e-6) * 100
        }
        
        # 2. å…‰æµä¸€è‡´æ€§
        flow_orig = self.flow_evaluator.compute_optical_flow_consistency(
            video_original, return_statistics=True
        )
        flow_def = self.flow_evaluator.compute_optical_flow_consistency(
            video_defended, return_statistics=True
        )
        
        results['metrics']['Optical_Flow_Consistency'] = {
            'original': flow_orig,
            'defended': flow_def
        }
        
        # 3. LPIPS
        lpips_orig = self.lpips_evaluator.compute_lpips(
            video_original, return_statistics=True
        )
        lpips_def = self.lpips_evaluator.compute_lpips(
            video_defended, return_statistics=True
        )
        
        results['metrics']['LPIPS'] = {
            'original': lpips_orig,
            'defended': lpips_def
        }
        
        if verbose:
            self._print_results(results)
        
        return results
    
    def _print_results(self, results: dict):
        """æ‰“å°æ ¼å¼åŒ–çš„è¯„ä¼°ç»“æœ"""
        
        print("\n" + "="*70)
        print(f"æç¤º: {results['text_prompt']}")
        print("="*70)
        
        # CLIP Score
        clip = results['metrics']['CLIP_Score']
        print(f"\nã€CLIP Scoreã€‘(è¶Šé«˜è¶Šå¥½ï¼ŒèŒƒå›´0-100)")
        print(f"  é˜²å¾¡å‰: {clip['original']:.2f}")
        print(f"  é˜²å¾¡å: {clip['defended']:.2f}")
        print(f"  å˜åŒ–:   {clip['delta']:+.2f} ({clip['delta_pct']:+.1f}%)")
        
        if abs(clip['delta']) < 5:
            status = "âœ“ æ— æ˜¾è‘—å½±å“"
        elif abs(clip['delta']) < 10:
            status = "âš  è½»å¾®å½±å“"
        else:
            status = "âœ— æ˜¾è‘—å½±å“"
        print(f"  åˆ¤å®š:   {status}")
        
        # å…‰æµä¸€è‡´æ€§
        flow = results['metrics']['Optical_Flow_Consistency']
        print(f"\nã€å…‰æµä¸€è‡´æ€§ã€‘(è¶Šé«˜è¶Šå¥½ï¼ŒèŒƒå›´0-1)")
        print(f"  é˜²å¾¡å‰: {flow['original']['consistency_score']:.4f}")
        print(f"  é˜²å¾¡å: {flow['defended']['consistency_score']:.4f}")
        print(f"  å˜åŒ–:   {flow['defended']['consistency_score'] - flow['original']['consistency_score']:+.4f}")
        
        # æ£€æŸ¥æ—¶é—´ä¼ªå½±
        cv_orig = flow['original']['variation_coefficient']
        cv_def = flow['defended']['variation_coefficient']
        print(f"  å…‰æµå¹…åº¦å˜å¼‚ç³»æ•°:")
        print(f"    é˜²å¾¡å‰: {cv_orig:.4f}")
        print(f"    é˜²å¾¡å: {cv_def:.4f}")
        if cv_def < cv_orig + 0.1:
            print(f"  åˆ¤å®š:   âœ“ æ— æ—¶é—´ä¼ªå½±")
        else:
            print(f"  åˆ¤å®š:   âœ— å¯èƒ½å­˜åœ¨å¸§é—ªçƒ")
        
        # LPIPS
        lpips = results['metrics']['LPIPS']
        print(f"\nã€LPIPSã€‘(è¶Šä½è¶Šå¥½ï¼ŒèŒƒå›´0-1)")
        print(f"  é˜²å¾¡å‰: {lpips['original']['mean_lpips']:.4f}")
        print(f"  é˜²å¾¡å: {lpips['defended']['mean_lpips']:.4f}")
        print(f"  å˜åŒ–:   {lpips['defended']['mean_lpips'] - lpips['original']['mean_lpips']:+.4f}")
        
        if abs(lpips['defended']['mean_lpips'] - lpips['original']['mean_lpips']) < 0.01:
            status = "âœ“ å¸§é—´è¿è´¯æ€§ä¿æŒè‰¯å¥½"
        else:
            status = "âš  å¸§é—´è¿è´¯æ€§æœ‰æ‰€ä¸‹é™"
        print(f"  åˆ¤å®š:   {status}")
        
        print("\n" + "="*70 + "\n")


# ============================================================================
# 5. å®ç”¨å·¥å…·å‡½æ•°
# ============================================================================

def load_video_from_file(video_path: str) -> torch.Tensor:
    """
    ä»æ–‡ä»¶åŠ è½½è§†é¢‘
    
    Args:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„ (.mp4, .avi, .mov ç­‰)
    
    Returns:
        torch.Tensor: (T, 3, H, W) æ ¼å¼ï¼ŒèŒƒå›´[0, 255]
    """
    import av
    
    container = av.open(video_path)
    frames = []
    
    for frame in container.decode(video=0):
        image = frame.to_ndarray(format='rgb24')
        frames.append(torch.from_numpy(image).permute(2, 0, 1))
    
    video_tensor = torch.stack(frames)
    return video_tensor


def save_evaluation_report(results: list, output_path: str = "evaluation_report.txt"):
    """
    ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ–‡ä»¶
    
    Args:
        results: è¯„ä¼°ç»“æœåˆ—è¡¨
        output_path: è¾“å‡ºè·¯å¾„
    """
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write("="*70 + "\n")
        f.write("T2Vå®‰å…¨Adapteré˜²å¾¡æ•ˆæœè¯„ä¼°æŠ¥å‘Š\n")
        f.write("="*70 + "\n\n")
        
        # ç»Ÿè®¡æ‘˜è¦
        clip_deltas = []
        lpips_deltas = []
        
        for result in results:
            clip = result['metrics']['CLIP_Score']
            lpips = result['metrics']['LPIPS']
            clip_deltas.append(clip['delta_pct'])
            lpips_deltas.append(lpips['defended']['mean_lpips'] - lpips['original']['mean_lpips'])
        
        f.write("ã€æ€»ä½“ç»Ÿè®¡ã€‘\n")
        f.write(f"è¯„ä¼°æ ·æœ¬æ•°: {len(results)}\n")
        f.write(f"CLIP Scoreå¹³å‡é™å¹…: {np.mean(clip_deltas):.2f}%\n")
        f.write(f"LPIPSå¹³å‡å˜åŒ–: {np.mean(lpips_deltas):+.4f}\n\n")
        
        # è¯¦ç»†ç»“æœ
        f.write("ã€è¯¦ç»†ç»“æœã€‘\n")
        for i, result in enumerate(results, 1):
            f.write(f"\næ ·æœ¬ {i}: {result['text_prompt']}\n")
            f.write("-" * 50 + "\n")
            
            clip = result['metrics']['CLIP_Score']
            f.write(f"CLIP Score: {clip['original']:.2f} -> {clip['defended']:.2f} ({clip['delta_pct']:+.1f}%)\n")
            
            lpips = result['metrics']['LPIPS']
            f.write(f"LPIPS: {lpips['original']['mean_lpips']:.4f} -> {lpips['defended']['mean_lpips']:.4f}\n")
    
    print(f"âœ“ è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜è‡³: {output_path}")


if __name__ == "__main__":
    # ========================================================================
    # æ‰¹é‡è¯„ä¼°çœŸå®æ•°æ®
    # ========================================================================
    import argparse
    import csv
    import os
    from pathlib import Path
    
    parser = argparse.ArgumentParser(description="T2Vå®‰å…¨Adapteré˜²å¾¡æ•ˆæœè¯„ä¼°")
    parser.add_argument(
        "--prompt_csv",
        type=str,
        default="/home/raykr/projects/PromptSafe-T2V/datasets/test/tiny/sexual.csv",
        help="åŒ…å«promptçš„CSVæ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--baseline_dir",
        type=str,
        default="/home/raykr/projects/PromptSafe-T2V/out/cogvideox-2b/benign/baseline",
        help="é˜²å¾¡å‰è§†é¢‘ç›®å½•ï¼ˆbaselineï¼‰"
    )
    parser.add_argument(
        "--defended_dir",
        type=str,
        default="/home/raykr/projects/PromptSafe-T2V/out/cogvideox-2b/benign/multi_defense",
        help="é˜²å¾¡åè§†é¢‘ç›®å½•ï¼ˆmulti_defenseï¼‰"
    )
    parser.add_argument(
        "--output_dir",
        type=str,
        default="./evaluation_results",
        help="è¯„ä¼°ç»“æœè¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--device",
        type=str,
        default="cuda",
        help="è®¡ç®—è®¾å¤‡ (cuda/cpu)"
    )
    parser.add_argument(
        "--max_samples",
        type=int,
        default=None,
        help="æœ€å¤§è¯„ä¼°æ ·æœ¬æ•°ï¼ˆNoneè¡¨ç¤ºå…¨éƒ¨ï¼‰"
    )
    parser.add_argument(
        "--baseline_pattern",
        type=str,
        default="adapter_{:03d}_raw.mp4",
        help="baselineè§†é¢‘æ–‡ä»¶å‘½åæ¨¡å¼"
    )
    parser.add_argument(
        "--defended_pattern",
        type=str,
        default="multi_{:03d}_safe.mp4",
        help="defendedè§†é¢‘æ–‡ä»¶å‘½åæ¨¡å¼"
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print("="*70)
    print("T2Vå®‰å…¨Adapteré˜²å¾¡æ•ˆæœè¯„ä¼° - æ‰¹é‡å¤„ç†")
    print("="*70)
    print(f"Prompt CSV: {args.prompt_csv}")
    print(f"Baselineç›®å½•: {args.baseline_dir}")
    print(f"Defendedç›®å½•: {args.defended_dir}")
    print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"è®¾å¤‡: {args.device}")
    print("="*70 + "\n")
    
    # 1. è¯»å–promptåˆ—è¡¨
    print("ğŸ“– è¯»å–promptåˆ—è¡¨...")
    prompts = []
    with open(args.prompt_csv, 'r', encoding='utf-8') as f:
        reader = csv.reader(f)
        next(reader)  # è·³è¿‡æ ‡é¢˜è¡Œ
        for row in reader:
            if row and len(row) > 0:
                prompt = row[0].strip().strip('"')  # ç§»é™¤å¼•å·
                if prompt:
                    prompts.append(prompt)
    
    if args.max_samples:
        prompts = prompts[:args.max_samples]
    
    print(f"âœ“ å…±è¯»å– {len(prompts)} ä¸ªprompt\n")
    
    # 2. åˆ›å»ºè¯„ä¼°å™¨
    print("ğŸ”§ åˆå§‹åŒ–è¯„ä¼°å™¨...")
    evaluator = T2VVideoEvaluator(device=args.device)
    print("âœ“ è¯„ä¼°å™¨åˆå§‹åŒ–å®Œæˆ\n")
    
    # 3. æ‰¹é‡è¯„ä¼°
    print("ğŸš€ å¼€å§‹æ‰¹é‡è¯„ä¼°...\n")
    all_results = []
    baseline_dir = Path(args.baseline_dir)
    defended_dir = Path(args.defended_dir)
    
    for idx, prompt in enumerate(prompts):
        print(f"[{idx+1}/{len(prompts)}] å¤„ç†æ ·æœ¬ {idx}")
        print(f"  Prompt: {prompt[:80]}..." if len(prompt) > 80 else f"  Prompt: {prompt}")
        
        # æ„å»ºè§†é¢‘æ–‡ä»¶è·¯å¾„
        baseline_video_path = baseline_dir / args.baseline_pattern.format(idx)
        defended_video_path = defended_dir / args.defended_pattern.format(idx)
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not baseline_video_path.exists():
            print(f"  âš  è·³è¿‡: baselineè§†é¢‘ä¸å­˜åœ¨ - {baseline_video_path}")
            continue
        if not defended_video_path.exists():
            print(f"  âš  è·³è¿‡: defendedè§†é¢‘ä¸å­˜åœ¨ - {defended_video_path}")
            continue
        
        try:
            # åŠ è½½è§†é¢‘
            print(f"  ğŸ“¹ åŠ è½½è§†é¢‘...")
            video_original = load_video_from_file(str(baseline_video_path))
            video_defended = load_video_from_file(str(defended_video_path))
            
            print(f"    Baseline: {video_original.shape}")
            print(f"    Defended: {video_defended.shape}")
            
            # è¯„ä¼°é˜²å¾¡å½±å“
            print(f"  ğŸ” è¯„ä¼°ä¸­...")
            result = evaluator.evaluate_defense_impact(
                video_original,
                video_defended,
                prompt,
                verbose=False  # æ‰¹é‡å¤„ç†æ—¶ä¸æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
            )
            
            # æ·»åŠ å…ƒæ•°æ®
            result['metadata'] = {
                'index': idx,
                'prompt': prompt,
                'baseline_video': str(baseline_video_path),
                'defended_video': str(defended_video_path)
            }
            
            all_results.append(result)
            
            # æ‰“å°ç®€è¦ç»“æœ
            clip_score = result['metrics']['CLIP_Score']
            print(f"  âœ“ CLIP Score: {clip_score['original']:.2f} -> {clip_score['defended']:.2f}")
            print()
            
        except Exception as e:
            print(f"  âŒ é”™è¯¯: {str(e)}\n")
            import traceback
            traceback.print_exc()
            continue
    
    # 4. ä¿å­˜è¯„ä¼°ç»“æœ
    print("="*70)
    print("ğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœ...")
    
    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    report_path = output_dir / "evaluation_report.txt"
    save_evaluation_report(all_results, str(report_path))
    
    # ä¿å­˜CSVæ ¼å¼çš„æ±‡æ€»ç»“æœ
    csv_path = output_dir / "evaluation_summary.csv"
    with open(csv_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow([
            'Index', 'Prompt', 
            'CLIP_Original', 'CLIP_Defended', 'CLIP_Diff',
            'Flow_Original', 'Flow_Defended', 'Flow_Diff',
            'LPIPS_Original', 'LPIPS_Defended', 'LPIPS_Diff'
        ])
        
        for result in all_results:
            meta = result['metadata']
            clip = result['metrics']['CLIP_Score']
            flow = result['metrics']['Optical_Flow_Consistency']
            lpips = result['metrics']['LPIPS']
            
            writer.writerow([
                meta['index'],
                meta['prompt'],
                f"{clip['original']:.4f}",
                f"{clip['defended']:.4f}",
                f"{clip['defended'] - clip['original']:.4f}",
                f"{flow['original']['consistency_score']:.4f}",
                f"{flow['defended']['consistency_score']:.4f}",
                f"{flow['defended']['consistency_score'] - flow['original']['consistency_score']:.4f}",
                f"{lpips['original']['mean_lpips']:.4f}",
                f"{lpips['defended']['mean_lpips']:.4f}",
                f"{lpips['defended']['mean_lpips'] - lpips['original']['mean_lpips']:.4f}",
            ])
    
    print(f"âœ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    print(f"âœ“ æ±‡æ€»CSVå·²ä¿å­˜: {csv_path}")
    
    # 5. æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*70)
    print("ğŸ“Š è¯„ä¼°ç»Ÿè®¡")
    print("="*70)
    
    if all_results:
        clip_diffs = [r['metrics']['CLIP_Score']['defended'] - r['metrics']['CLIP_Score']['original'] 
                     for r in all_results]
        flow_diffs = [r['metrics']['Optical_Flow_Consistency']['defended']['consistency_score'] - r['metrics']['Optical_Flow_Consistency']['original']['consistency_score']
                     for r in all_results]
        lpips_diffs = [r['metrics']['LPIPS']['defended']['mean_lpips'] - r['metrics']['LPIPS']['original']['mean_lpips']
                      for r in all_results]
        
        print(f"\nCLIP Scoreå˜åŒ–:")
        print(f"  å¹³å‡: {np.mean(clip_diffs):.4f}")
        print(f"  ä¸­ä½æ•°: {np.median(clip_diffs):.4f}")
        print(f"  æ ‡å‡†å·®: {np.std(clip_diffs):.4f}")
        
        print(f"\nå…‰æµä¸€è‡´æ€§å˜åŒ–:")
        print(f"  å¹³å‡: {np.mean(flow_diffs):.4f}")
        print(f"  ä¸­ä½æ•°: {np.median(flow_diffs):.4f}")
        print(f"  æ ‡å‡†å·®: {np.std(flow_diffs):.4f}")
        
        print(f"\nLPIPSå˜åŒ–:")
        print(f"  å¹³å‡: {np.mean(lpips_diffs):.4f}")
        print(f"  ä¸­ä½æ•°: {np.median(lpips_diffs):.4f}")
        print(f"  æ ‡å‡†å·®: {np.std(lpips_diffs):.4f}")
    
    print(f"\nâœ“ å…±æˆåŠŸè¯„ä¼° {len(all_results)}/{len(prompts)} ä¸ªæ ·æœ¬")
    print("="*70)
