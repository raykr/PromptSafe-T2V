import torch
import torch.nn.functional as F
from transformers import T5EncoderModel, AutoTokenizer
from tqdm import tqdm
import time
from torch.utils.tensorboard import SummaryWriter
import os


class SafeTextEncoderTrainer:
    def __init__(self, model_name, placeholder_token="<safe>", initializer_token="safe", num_vectors=1, device="cuda", log_dir="runs"):
        self.device = device
        self.num_vectors = num_vectors
        self.placeholder_token = placeholder_token
        self.initializer_token = initializer_token
        self.log_dir = log_dir
        
        # åˆ›å»ºTensorBoard writer
        os.makedirs(log_dir, exist_ok=True)
        self.writer = SummaryWriter(log_dir)

        # 1. åŠ è½½é¢„è®­ç»ƒ T5Encoder
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, subfolder="tokenizer")
        self.text_encoder = T5EncoderModel.from_pretrained(model_name, subfolder="text_encoder")
        self.text_encoder.to(device)
        
        # 2. è®¾ç½® placeholder tokens
        self.setup_placeholder_tokens()

        # 3. å†»ç»“ text_model çš„å‚æ•°
        self.text_encoder.requires_grad_(False)
        

        # åªè®­ç»ƒ soft token embedding
        # åˆ›å»ºå¯è®­ç»ƒçš„soft token embedding
        self.soft_token_embedding = self.text_encoder.get_input_embeddings().weight[self.placeholder_token_ids].clone().detach().requires_grad_(True)
        self.optimizer = torch.optim.AdamW([self.soft_token_embedding], lr=5e-4)

        

    def encode(self, texts):
        token_ids = self.tokenizer(texts, padding=True, truncation=True, return_tensors="pt").input_ids
        token_ids = token_ids.to(self.device)  # å°†token_idsç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡
        return self.text_encoder(token_ids)[0]  # [B, L, D]

    def training_step(self, malicious, rewritten, benign=None, lambda_benign=0.1):
        # 1. åŠ ä¸Š <safe> soft token
        malicious_safe = [f"{self.placeholder_token} {t}" for t in malicious]

        # 2. ç¼–ç  - ä½¿ç”¨è‡ªå®šä¹‰embedding
        emb_mal_safe = self._encode_with_soft_token(malicious_safe).mean(dim=1)  # [B, D]
        emb_rewritten = self.encode(rewritten).mean(dim=1)  # [B, D]

        # 3. ä¸» lossï¼šå¯¹é½ malicious+<safe> åˆ° rewritten
        align_loss = F.mse_loss(emb_mal_safe, emb_rewritten)

        # 4. benign consistencyï¼ˆå¯é€‰ï¼‰
        if benign is not None:
            emb_benign = self.encode(benign).mean(dim=1)
            emb_benign_safe = self._encode_with_soft_token([f"{self.placeholder_token} {t}" for t in benign]).mean(dim=1)
            benign_loss = F.mse_loss(emb_benign, emb_benign_safe)
        else:
            benign_loss = torch.tensor(0.0, device=self.device)

        loss = align_loss + lambda_benign * benign_loss

        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()

        return {"loss": loss.item(), "align_loss": align_loss.item(), "benign_loss": benign_loss.item()}
    
    def train(self, malicious, rewritten, benign=None, lambda_benign=0.1, num_steps=1000, log_interval=10, save_interval=100):
        """è®­ç»ƒæ–¹æ³•ï¼ŒåŒ…å«è¿›åº¦æ¡å’ŒTensorBoardæ—¥å¿—è®°å½•"""
        print(f"ğŸš€ å¼€å§‹è®­ç»ƒï¼Œå…± {num_steps} æ­¥...")
        print(f"ğŸ“Š è®­ç»ƒå‚æ•°: lambda_benign={lambda_benign}, log_interval={log_interval}")
        print(f"ğŸ“ˆ TensorBoardæ—¥å¿—ä¿å­˜åˆ°: {self.log_dir}")
        print("-" * 60)
        
        # åˆå§‹åŒ–ç»Ÿè®¡ä¿¡æ¯
        total_loss = 0.0
        total_align_loss = 0.0
        total_benign_loss = 0.0
        best_loss = float('inf')
        
        # è®°å½•è®­ç»ƒå¼€å§‹æ—¶é—´
        self.writer.add_text("è®­ç»ƒé…ç½®", f"num_steps={num_steps}, lambda_benign={lambda_benign}, log_interval={log_interval}")
        
        # åˆ›å»ºè¿›åº¦æ¡
        pbar = tqdm(range(num_steps), desc="è®­ç»ƒè¿›åº¦", unit="step", 
                   bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]")
        
        start_time = time.time()
        
        for step in pbar:
            # æ‰§è¡Œè®­ç»ƒæ­¥éª¤
            logs = self.training_step(malicious, rewritten, benign, lambda_benign)
            
            # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
            total_loss += logs["loss"]
            total_align_loss += logs["align_loss"]
            total_benign_loss += logs["benign_loss"]
            
            # æ›´æ–°æœ€ä½³loss
            if logs["loss"] < best_loss:
                best_loss = logs["loss"]
            
            # è®°å½•åˆ°TensorBoard
            self.writer.add_scalar("Loss/Total", logs["loss"], step)
            self.writer.add_scalar("Loss/Align", logs["align_loss"], step)
            self.writer.add_scalar("Loss/Benign", logs["benign_loss"], step)
            self.writer.add_scalar("Loss/Best", best_loss, step)
            
            # è®°å½•å­¦ä¹ ç‡
            current_lr = self.optimizer.param_groups[0]['lr']
            self.writer.add_scalar("Learning_Rate", current_lr, step)
            
            # è®°å½•å¹³å‡æŸå¤±
            avg_loss = total_loss / (step + 1)
            avg_align = total_align_loss / (step + 1)
            avg_benign = total_benign_loss / (step + 1)
            self.writer.add_scalar("Loss/Average_Total", avg_loss, step)
            self.writer.add_scalar("Loss/Average_Align", avg_align, step)
            self.writer.add_scalar("Loss/Average_Benign", avg_benign, step)
            
            # æ›´æ–°è¿›åº¦æ¡æè¿°
            pbar.set_postfix({
                'Loss': f"{logs['loss']:.6f}",
                'Align': f"{logs['align_loss']:.6f}",
                'Benign': f"{logs['benign_loss']:.6f}",
                'Best': f"{best_loss:.6f}"
            })
            
            # å®šæœŸæ‰“å°è¯¦ç»†æ—¥å¿—
            if step % log_interval == 0:
                elapsed_time = time.time() - start_time
                
                print(f"\nğŸ“ˆ æ­¥éª¤ {step:4d} | "
                      f"Loss: {logs['loss']:.6f} (avg: {avg_loss:.6f}) | "
                      f"Align: {logs['align_loss']:.6f} (avg: {avg_align:.6f}) | "
                      f"Benign: {logs['benign_loss']:.6f} (avg: {avg_benign:.6f}) | "
                      f"æ—¶é—´: {elapsed_time:.1f}s")
                
                # è®°å½•è®­ç»ƒé€Ÿåº¦
                steps_per_sec = (step + 1) / elapsed_time
                self.writer.add_scalar("Training/Speed", steps_per_sec, step)
            
            # å®šæœŸä¿å­˜æ¨¡å‹
            if step % save_interval == 0 and step > 0:
                checkpoint_path = f"checkpoint_step_{step}.safetensors"
                self.save_soft_embedding(checkpoint_path)
                print(f"ğŸ’¾ å·²ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")
                
                # è®°å½•æ£€æŸ¥ç‚¹ä¿å­˜
                self.writer.add_text("æ£€æŸ¥ç‚¹", f"æ­¥éª¤ {step}: {checkpoint_path}")
        
        # è®­ç»ƒå®Œæˆç»Ÿè®¡
        total_time = time.time() - start_time
        avg_loss = total_loss / num_steps
        avg_align = total_align_loss / num_steps
        avg_benign = total_benign_loss / num_steps
        
        # è®°å½•æœ€ç»ˆç»Ÿè®¡åˆ°TensorBoard
        self.writer.add_text("è®­ç»ƒç»“æœ", f"æ€»è®­ç»ƒæ—¶é—´: {total_time:.2f}ç§’")
        self.writer.add_scalar("Final/Average_Total_Loss", avg_loss, num_steps)
        self.writer.add_scalar("Final/Average_Align_Loss", avg_align, num_steps)
        self.writer.add_scalar("Final/Average_Benign_Loss", avg_benign, num_steps)
        self.writer.add_scalar("Final/Best_Loss", best_loss, num_steps)
        self.writer.add_scalar("Final/Total_Time", total_time, num_steps)
        
        print("\n" + "="*60)
        print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_time:.2f}ç§’")
        print(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"   - å¹³å‡æ€»Loss: {avg_loss:.6f}")
        print(f"   - å¹³å‡å¯¹é½Loss: {avg_align:.6f}")
        print(f"   - å¹³å‡è‰¯æ€§Loss: {avg_benign:.6f}")
        print(f"   - æœ€ä½³Loss: {best_loss:.6f}")
        print(f"ğŸ“ˆ TensorBoardæ—¥å¿—å·²ä¿å­˜åˆ°: {self.log_dir}")
        print("="*60)
    
    def _encode_with_soft_token(self, texts):
        """ä½¿ç”¨soft tokenè¿›è¡Œç¼–ç """
        token_ids = self.tokenizer(texts, padding=True, truncation=True, return_tensors="pt").input_ids.to(self.device)
        
        # è·å–åŸå§‹embeddingçŸ©é˜µ
        embedding_matrix = self.text_encoder.get_input_embeddings().weight
        
        # åˆ›å»ºåŒ…å«soft tokençš„embeddingçŸ©é˜µ
        custom_embeddings = embedding_matrix.clone()
        custom_embeddings[self.placeholder_token_ids] = self.soft_token_embedding
        
        # æ‰‹åŠ¨è®¡ç®—embeddings
        inputs_embeds = F.embedding(token_ids, custom_embeddings)
        
        # é€šè¿‡T5 encoder
        encoder_outputs = self.text_encoder.encoder(
            inputs_embeds=inputs_embeds,
            attention_mask=None,  # ç®€åŒ–å¤„ç†
        )
        
        return encoder_outputs.last_hidden_state

    def save_soft_embedding(self, path="learned_embeds.safetensors"):
        from safetensors.torch import save_file

        state_dict = {
            self.placeholder_token: self.soft_token_embedding.detach().cpu()
        }
        save_file(state_dict, path)
        print(f"âœ… Soft token embedding saved to {path}")
    
    def close_tensorboard(self):
        """å…³é—­TensorBoard writer"""
        if hasattr(self, 'writer'):
            self.writer.close()
            print(f"ğŸ“ˆ TensorBoard writerå·²å…³é—­")

    def setup_placeholder_tokens(self):
        placeholder_tokens = [self.placeholder_token]
        for i in range(1, self.num_vectors):
            placeholder_tokens.append(f"{self.placeholder_token}_{i}")

        num_added_tokens = self.tokenizer.add_tokens(placeholder_tokens)
        if num_added_tokens != self.num_vectors:
            raise ValueError(
                f"The tokenizer already contains the token {self.placeholder_token}. Please pass a different"
                " `placeholder_token` that is not already in the tokenizer."
            )

        token_ids = self.tokenizer.encode(self.initializer_token, add_special_tokens=False)
        if len(token_ids) > 1:
            raise ValueError("The initializer token must be a single token.")

        self.initializer_token_id = token_ids[0]
        self.placeholder_token_ids = self.tokenizer.convert_tokens_to_ids(placeholder_tokens)

        self.text_encoder.resize_token_embeddings(len(self.tokenizer))
        token_embeds = self.text_encoder.get_input_embeddings().weight.data
        with torch.no_grad():
            for token_id in self.placeholder_token_ids:
                token_embeds[token_id] = token_embeds[self.initializer_token_id].clone()


trainer = SafeTextEncoderTrainer(
    model_name="/home/beihang/jzl/models/zai-org/CogVideoX-2b",
    placeholder_token="<safe>",
    initializer_token="safe",
    num_vectors=16,
    device="cuda",
    log_dir="runs/safe_text_encoder_training"
)

# æ¨¡æ‹Ÿæ•°æ®
malicious = ["a nude woman is running in the street"]
rewritten = ["a woman with red clothes is running in the street"]
benign = ["a man is running in the park"]

# ä½¿ç”¨æ–°çš„è®­ç»ƒæ–¹æ³•ï¼ˆåŒ…å«è¿›åº¦æ¡ï¼‰
trainer.train(
    malicious=malicious,
    rewritten=rewritten,
    benign=None,
    lambda_benign=0.1,
    num_steps=10000,
    log_interval=50,  # æ¯50æ­¥æ‰“å°ä¸€æ¬¡è¯¦ç»†æ—¥å¿—
    save_interval=500  # æ¯200æ­¥ä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
)

# ä¿å­˜æœ€ç»ˆæ¨¡å‹
trainer.save_soft_embedding("learned_safe.safetensors")

# å…³é—­TensorBoard writer
trainer.close_tensorboard()
